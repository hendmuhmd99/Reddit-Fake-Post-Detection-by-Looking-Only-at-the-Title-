{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3a36e53",
   "metadata": {},
   "source": [
    "# What is the input?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e755ff",
   "metadata": {},
   "source": [
    "Text dataset that has alot of noise and false information and the existence of such data on the internet caused many social problems as it has a role on many different domains, the data needs alot of cleaning and preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295ec29e",
   "metadata": {},
   "source": [
    "# What is the output?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb05986",
   "metadata": {},
   "source": [
    " prediction if a specific post is fake or not, by looking at its title. the fake news' label is (0) and the real news will be labeled by (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22611ca",
   "metadata": {},
   "source": [
    "# What data mining function is required?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68593562",
   "metadata": {},
   "source": [
    "binary classification with different models using text preprocessing techniques, pipelines,Random Search using a validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33734f11",
   "metadata": {},
   "source": [
    "# What could be the challenges?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40afefe8",
   "metadata": {},
   "source": [
    "- In order to extract information from big data, we must ensure that our method is efficient and scalable, and we must have adequate knowledge and expertise to use them if we need to update our algorithms.\n",
    "\n",
    "- Complex data that can be found in datasets.\n",
    "\n",
    "- Creating a viable solution to our issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f51b38f",
   "metadata": {},
   "source": [
    "# What is the impact?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fa5907",
   "metadata": {},
   "source": [
    "Our model predictions is going to predict wether a certain post is fake or not and this will greatly reduce the social problems caused by those fake news as many people believe them and they act according to them while in reality, they are not true at all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fae5feb",
   "metadata": {},
   "source": [
    "# What is an ideal solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd05b6e",
   "metadata": {},
   "source": [
    "In my perspective, an ideal solution will be measured in terms of metrics and results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07912f96",
   "metadata": {},
   "source": [
    "# What is the experimental protocol used and how was it carried out? What preprocessing steps are used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61c5c24",
   "metadata": {},
   "source": [
    "when it comes to the preprocessing steps I made a function performing the following on the training set and testing set :\n",
    "\n",
    " remove any html tags, Keep only ASCII + European Chars and whitespace, remove single letter chars,convert all whitespaces  to single wspace.\n",
    " \n",
    "if not for embedding :\n",
    "\n",
    "all lowercase\n",
    "\n",
    "remove stopwords, punctuation and stemm\n",
    "\n",
    "stemming\n",
    "\n",
    "\n",
    "The experimental protocol I used in my code is validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe71e41",
   "metadata": {},
   "source": [
    "# What is the difference between Character n-gram and Word n-gram? Which one tends to suffer more from the OOV issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea4f897",
   "metadata": {},
   "source": [
    "When comparing two linguistic versions, character N-grams or word N-grams. The alignment systems are traditionally based on word\n",
    "N-grams splitting. The observation of the morphological variety of languages, even inside a single linguistic family, quickly shows\n",
    "that the word granularity is inadequate to provide a widely multilingual system, i.e. a language independent system able to handle\n",
    "flexional languages as well as positional languages. Instead, when starting from a multilingual collection to focus on pairs of texts,\n",
    "we defend that character N-grams alignment is more efficient than word N-grams alignment\n",
    "\n",
    "the word n-gram tends to suffer more from the OOV issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e29e861",
   "metadata": {},
   "source": [
    "# What is the difference between stop word removal and stemming? Are these techniques language-dependent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85850c34",
   "metadata": {},
   "source": [
    "Stop word removal is the process of deleting terms that appear often in all of the papers in the corpus. Articles and pronouns are typically categorised as stop words.\n",
    "\n",
    "Stemming is the practise of reducing word inflection to its basic forms, such as mapping a group of words to the same stem, even if the stem isn't a valid word in the language.\n",
    "\n",
    "both stemming and stop word removal are language-dependant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae378b08",
   "metadata": {},
   "source": [
    " # Is tokenization techniques language dependent? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaedcd90",
   "metadata": {},
   "source": [
    "Tokenization is the process of breaking down a huge body of information into smaller lines, words, or even inventing new terms for a language other than English.\n",
    "\n",
    "You should always tokenize document and query words the same way, whether you're using a Boolean or a free text inquiry, and process inquiries with the same tokenizer. This ensures that a string of characters in a text will always match a string of characters typed in a query.\n",
    "These tokenization difficulties are language-specific. As a result, it is necessary to understand the document's language. It is quite successful to identify languages using classifiers that employ short character subsequences as features; most languages have distinct signature patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02049df",
   "metadata": {},
   "source": [
    "# What is the difference between count vectorizer and tf-idf vectorizer? Would it be feasible to use all possible n-grams? If not, how should you select them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2876d9",
   "metadata": {},
   "source": [
    "The TfidfTransformer transforms a count matrix to a normalized tf or tf-idf representation. So although both the CountVectorizer and TfidfTransformer (with use_idf=False) produce term frequencies, TfidfTransformer is normalizing the count.\n",
    "\n",
    "'n' is simply a variable with positive integer values such as 1,2,3, and so on. The letter 'n' stands for many. In a similar vein, depending on the value of 'n,' n-grams are categorised into the following categories.\n",
    "\n",
    "1:Unigram\n",
    "\n",
    "2:Bigram\n",
    "\n",
    "3:Trigram\n",
    "\n",
    "n:n-gram\n",
    "\n",
    "No it won't be be feasible to use all possible n-grams\n",
    "\n",
    "we need many different types of n-grams,This is because different types of n-grams are suitable for different types of applications. You should try different n-grams on your data in order to confidently conclude which one works the best among all for your text analysis.\n",
    "\n",
    "A 1-gram (or unigram) is a one-word sequence\n",
    "\n",
    "A2gram (or bigram) is a two-word sequence of words\n",
    " \n",
    "Unigram can provide better test results than Bigram and Trigram\n",
    "\n",
    "N-gram range sets if features to be used to characterize texts will be:\n",
    "\n",
    "Unigrams or words (n-gram size = 1)\n",
    "\n",
    "Bigrams or terms compounded by two words (n-gram size = 2)\n",
    "\n",
    "Trigrams or terms compounded by up to three words (n-gram size = 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aea02fc",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a64b0fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #I imported pandas cause Using it\n",
    "#make it way easier when it comes to  processing, analysis and manipulation of data\n",
    "import numpy as np # as it has array Manipulation functions\n",
    "from nltk.stem.snowball import SnowballStemmer#stemmer converts words to its root form\n",
    "from nltk.tokenize import word_tokenize # we will be able to extract the tokens from string of characters\n",
    "from nltk.corpus import stopwords # imported it to be able to remove stop words\n",
    "import nltk #The Natural Language Toolkit which are libraries and programs for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54dd67c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>265723</th>\n",
       "      <td>A group of friends began to volunteer at a hom...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284269</th>\n",
       "      <td>British Prime Minister @Theresa_May on Nerve A...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207715</th>\n",
       "      <td>In 1961, Goodyear released a kit that allows P...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551106</th>\n",
       "      <td>Happy Birthday, Bob Barker! The Price Is Right...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8584</th>\n",
       "      <td>Obama to Nation: 聙\"Innocent Cops and Unarmed Y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70046</th>\n",
       "      <td>Finish Sniper Simo H盲yh盲 during the invasion o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189377</th>\n",
       "      <td>Nigerian Prince Scam took $110K from Kansas ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93486</th>\n",
       "      <td>Is It Safe To Smoke Marijuana During Pregnancy...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140950</th>\n",
       "      <td>Julius Caesar upon realizing that everyone in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34509</th>\n",
       "      <td>Jeff Bridges Releasing 鈥楽leeping Tapes,鈥?a New...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "id                                                              \n",
       "265723  A group of friends began to volunteer at a hom...      0\n",
       "284269  British Prime Minister @Theresa_May on Nerve A...      0\n",
       "207715  In 1961, Goodyear released a kit that allows P...      0\n",
       "551106  Happy Birthday, Bob Barker! The Price Is Right...      0\n",
       "8584    Obama to Nation: 聙\"Innocent Cops and Unarmed Y...      0\n",
       "...                                                   ...    ...\n",
       "70046   Finish Sniper Simo H盲yh盲 during the invasion o...      0\n",
       "189377  Nigerian Prince Scam took $110K from Kansas ma...      1\n",
       "93486   Is It Safe To Smoke Marijuana During Pregnancy...      0\n",
       "140950  Julius Caesar upon realizing that everyone in ...      0\n",
       "34509   Jeff Bridges Releasing 鈥楽leeping Tapes,鈥?a New...      1\n",
       "\n",
       "[60000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traind=pd.read_csv('xy_train.csv',index_col='id')#I imported my training data set\n",
    "# and Set column \"id\" as index column\n",
    "traind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ae97ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stargazer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yeah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PD: Phoenix car thief gets instructions from Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As Trump Accuses Iran, He Has One Problem: His...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Believers\" - Hezbollah 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59146</th>\n",
       "      <td>Bicycle taxi drivers of New Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59147</th>\n",
       "      <td>Trump blows up GOP's formula for winning House...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59148</th>\n",
       "      <td>Napoleon returns from his exile on the island ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59149</th>\n",
       "      <td>Deep down he always wanted to be a ballet dancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59150</th>\n",
       "      <td>Toddler miraculously survives 6-story fall lan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59151 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "id                                                      \n",
       "0                                             stargazer \n",
       "1                                                   yeah\n",
       "2      PD: Phoenix car thief gets instructions from Y...\n",
       "3      As Trump Accuses Iran, He Has One Problem: His...\n",
       "4                           \"Believers\" - Hezbollah 2011\n",
       "...                                                  ...\n",
       "59146                  Bicycle taxi drivers of New Delhi\n",
       "59147  Trump blows up GOP's formula for winning House...\n",
       "59148  Napoleon returns from his exile on the island ...\n",
       "59149   Deep down he always wanted to be a ballet dancer\n",
       "59150  Toddler miraculously survives 6-story fall lan...\n",
       "\n",
       "[59151 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testd=pd.read_csv('x_test.csv',index_col='id')\n",
    "testd  #I imported my testing data set\n",
    "# Set column \"id\" as index column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dffabc24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>265723</th>\n",
       "      <td>A group of friends began to volunteer at a hom...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284269</th>\n",
       "      <td>British Prime Minister @Theresa_May on Nerve A...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207715</th>\n",
       "      <td>In 1961, Goodyear released a kit that allows P...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551106</th>\n",
       "      <td>Happy Birthday, Bob Barker! The Price Is Right...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8584</th>\n",
       "      <td>Obama to Nation: 聙\"Innocent Cops and Unarmed Y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117912</th>\n",
       "      <td>In the 1920鈥檚, Hitler was forbidden to address...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213064</th>\n",
       "      <td>Nerd Wins Scrabble with word you've never hear...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398923</th>\n",
       "      <td>Why 95.8% of Female Newscasters Have the Same ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314798</th>\n",
       "      <td>Donald Trump Says He'll Do This If More 'Inapp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20243</th>\n",
       "      <td>5 crazy facts about Lamborghini's outrageous e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "id                                                              \n",
       "265723  A group of friends began to volunteer at a hom...      0\n",
       "284269  British Prime Minister @Theresa_May on Nerve A...      0\n",
       "207715  In 1961, Goodyear released a kit that allows P...      0\n",
       "551106  Happy Birthday, Bob Barker! The Price Is Right...      0\n",
       "8584    Obama to Nation: 聙\"Innocent Cops and Unarmed Y...      0\n",
       "117912  In the 1920鈥檚, Hitler was forbidden to address...      0\n",
       "213064  Nerd Wins Scrabble with word you've never hear...      0\n",
       "398923  Why 95.8% of Female Newscasters Have the Same ...      1\n",
       "314798  Donald Trump Says He'll Do This If More 'Inapp...      0\n",
       "20243   5 crazy facts about Lamborghini's outrageous e...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traind.head(10)#view first 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41e26eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     False\n",
       "label    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traind.isnull().any() #checking if there are any nulls in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "565e145d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testd.isnull().any() #checking if there are any nulls in testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be7a5afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Debi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Debi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt') #downloading the tokenizer which divides a text into a list of sentences from nltk\n",
    "nltk.download('stopwords')#Stop words are words that are going to be ignored by tokenizers.\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\") #stemmer converts words to its root form, for english language\n",
    "stop_words = set(stopwords.words(\"english\"))#to be able to remove stop words for english language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb65905f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, for_embedding=False): #defining a function for cleaning \n",
    "    \"\"\" steps:\n",
    "        - remove any html tags (< /br> often found)\n",
    "        - Keep only ASCII + European Chars and whitespace, no digits\n",
    "        - remove single letter chars\n",
    "        - convert all whitespaces (tabs etc.) to single wspace\n",
    "        if not for embedding :\n",
    "        - all lowercase\n",
    "        - remove stopwords, punctuation and stemm\n",
    "    \"\"\"\n",
    "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE) #for converting whitespacs to single white space\n",
    "    RE_TAGS = re.compile(r\"<[^>]+>\") #for removing html tags\n",
    "    RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž ]\", re.IGNORECASE) #removing ASCII chars and european chars\n",
    "    RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE) # removing single letter chars\n",
    "    if for_embedding: #   if not for embedding :\n",
    "        # - all lowercase\n",
    "        # - remove stopwords, punctuation and stemm\n",
    "        # Keep punctuation\n",
    "        RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž,.!? ]\", re.IGNORECASE)\n",
    "        RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž,.!?]\\b\", re.IGNORECASE)\n",
    "\n",
    "    text = re.sub(RE_TAGS, \" \", text)\n",
    "    text = re.sub(RE_ASCII, \" \", text)\n",
    "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
    "    text = re.sub(RE_WSPACE, \" \", text)\n",
    "\n",
    "    word_tokens = word_tokenize(text) ## to extract the tokens from string of characters\n",
    "    words_tokens_lower = [word.lower() for word in word_tokens]\n",
    "\n",
    "    if for_embedding:\n",
    "        # no stemming, lowering and punctuation / stop words removal\n",
    "        words_filtered = word_tokens\n",
    "    else:\n",
    "        words_filtered = [\n",
    "            stemmer.stem(word) for word in words_tokens_lower if word not in stop_words\n",
    "        ]\n",
    "\n",
    "    text_clean = \" \".join(words_filtered)\n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a384e0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "import re #The power of regular expressions is that they can specify patterns, not just fixed characters.\n",
    "\n",
    "traind[\"text_clean\"] = traind.loc[traind[\"text\"].str.len() > 4, \"text\"] #defining a text clean column in training set\n",
    "traind[\"text_clean\"] = traind[\"text\"].map( #applying the text_clean function on the 'text' column and putting the results in the text_clean column \n",
    "    lambda x: clean_text(x, for_embedding=False) if isinstance(x, str) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "375ea639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>265723</th>\n",
       "      <td>A group of friends began to volunteer at a hom...</td>\n",
       "      <td>0</td>\n",
       "      <td>group friend began volunt homeless shelter nei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284269</th>\n",
       "      <td>British Prime Minister @Theresa_May on Nerve A...</td>\n",
       "      <td>0</td>\n",
       "      <td>british prime minist theresa may nerv attack f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207715</th>\n",
       "      <td>In 1961, Goodyear released a kit that allows P...</td>\n",
       "      <td>0</td>\n",
       "      <td>goodyear releas kit allow ps brought heel http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551106</th>\n",
       "      <td>Happy Birthday, Bob Barker! The Price Is Right...</td>\n",
       "      <td>0</td>\n",
       "      <td>happi birthday bob barker price right host lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8584</th>\n",
       "      <td>Obama to Nation: 聙\"Innocent Cops and Unarmed Y...</td>\n",
       "      <td>0</td>\n",
       "      <td>obama nation innoc cop unarm young black men d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70046</th>\n",
       "      <td>Finish Sniper Simo H盲yh盲 during the invasion o...</td>\n",
       "      <td>0</td>\n",
       "      <td>finish sniper simo yh invas finland ussr color</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189377</th>\n",
       "      <td>Nigerian Prince Scam took $110K from Kansas ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>nigerian princ scam took kansa man year later ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93486</th>\n",
       "      <td>Is It Safe To Smoke Marijuana During Pregnancy...</td>\n",
       "      <td>0</td>\n",
       "      <td>safe smoke marijuana pregnanc surpris answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140950</th>\n",
       "      <td>Julius Caesar upon realizing that everyone in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>julius caesar upon realiz everyon room knife e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34509</th>\n",
       "      <td>Jeff Bridges Releasing 鈥楽leeping Tapes,鈥?a New...</td>\n",
       "      <td>1</td>\n",
       "      <td>jeff bridg releas leep tape new album design h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label  \\\n",
       "id                                                                 \n",
       "265723  A group of friends began to volunteer at a hom...      0   \n",
       "284269  British Prime Minister @Theresa_May on Nerve A...      0   \n",
       "207715  In 1961, Goodyear released a kit that allows P...      0   \n",
       "551106  Happy Birthday, Bob Barker! The Price Is Right...      0   \n",
       "8584    Obama to Nation: 聙\"Innocent Cops and Unarmed Y...      0   \n",
       "...                                                   ...    ...   \n",
       "70046   Finish Sniper Simo H盲yh盲 during the invasion o...      0   \n",
       "189377  Nigerian Prince Scam took $110K from Kansas ma...      1   \n",
       "93486   Is It Safe To Smoke Marijuana During Pregnancy...      0   \n",
       "140950  Julius Caesar upon realizing that everyone in ...      0   \n",
       "34509   Jeff Bridges Releasing 鈥楽leeping Tapes,鈥?a New...      1   \n",
       "\n",
       "                                               text_clean  \n",
       "id                                                         \n",
       "265723  group friend began volunt homeless shelter nei...  \n",
       "284269  british prime minist theresa may nerv attack f...  \n",
       "207715  goodyear releas kit allow ps brought heel http...  \n",
       "551106  happi birthday bob barker price right host lik...  \n",
       "8584    obama nation innoc cop unarm young black men d...  \n",
       "...                                                   ...  \n",
       "70046      finish sniper simo yh invas finland ussr color  \n",
       "189377  nigerian princ scam took kansa man year later ...  \n",
       "93486        safe smoke marijuana pregnanc surpris answer  \n",
       "140950  julius caesar upon realiz everyon room knife e...  \n",
       "34509   jeff bridg releas leep tape new album design h...  \n",
       "\n",
       "[60000 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traind #diplaying the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c23f3ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "testd[\"text_clean\"] = testd.loc[testd[\"text\"].str.len() > 4, \"text\"] #defining a text clean column in testing set\n",
    "testd[\"text_clean\"] = testd[\"text\"].map(  #applying the text_clean function on the 'text' column and putting the results in the text_clean column\n",
    "    lambda x: clean_text(x, for_embedding=False) if isinstance(x, str) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5747f5f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stargazer</td>\n",
       "      <td>stargaz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yeah</td>\n",
       "      <td>yeah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PD: Phoenix car thief gets instructions from Y...</td>\n",
       "      <td>pd phoenix car thief get instruct youtub video</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As Trump Accuses Iran, He Has One Problem: His...</td>\n",
       "      <td>trump accus iran one problem credibl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Believers\" - Hezbollah 2011</td>\n",
       "      <td>believ hezbollah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59146</th>\n",
       "      <td>Bicycle taxi drivers of New Delhi</td>\n",
       "      <td>bicycl taxi driver new delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59147</th>\n",
       "      <td>Trump blows up GOP's formula for winning House...</td>\n",
       "      <td>trump blow gop formula win hous race</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59148</th>\n",
       "      <td>Napoleon returns from his exile on the island ...</td>\n",
       "      <td>napoleon return exil island elba march colouris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59149</th>\n",
       "      <td>Deep down he always wanted to be a ballet dancer</td>\n",
       "      <td>deep alway want ballet dancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59150</th>\n",
       "      <td>Toddler miraculously survives 6-story fall lan...</td>\n",
       "      <td>toddler miracul surviv stori fall land car</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59151 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "id                                                         \n",
       "0                                             stargazer    \n",
       "1                                                   yeah   \n",
       "2      PD: Phoenix car thief gets instructions from Y...   \n",
       "3      As Trump Accuses Iran, He Has One Problem: His...   \n",
       "4                           \"Believers\" - Hezbollah 2011   \n",
       "...                                                  ...   \n",
       "59146                  Bicycle taxi drivers of New Delhi   \n",
       "59147  Trump blows up GOP's formula for winning House...   \n",
       "59148  Napoleon returns from his exile on the island ...   \n",
       "59149   Deep down he always wanted to be a ballet dancer   \n",
       "59150  Toddler miraculously survives 6-story fall lan...   \n",
       "\n",
       "                                            text_clean  \n",
       "id                                                      \n",
       "0                                              stargaz  \n",
       "1                                                 yeah  \n",
       "2       pd phoenix car thief get instruct youtub video  \n",
       "3                 trump accus iran one problem credibl  \n",
       "4                                     believ hezbollah  \n",
       "...                                                ...  \n",
       "59146                     bicycl taxi driver new delhi  \n",
       "59147             trump blow gop formula win hous race  \n",
       "59148  napoleon return exil island elba march colouris  \n",
       "59149                    deep alway want ballet dancer  \n",
       "59150       toddler miracul surviv stori fall land car  \n",
       "\n",
       "[59151 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testd # displaying the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82983971",
   "metadata": {},
   "outputs": [],
   "source": [
    "traind.loc[traind[\"label\"] > 1] = np.NaN #deleting any label that is not 0 or 1\n",
    "\n",
    "# Drop when any of x missing\n",
    "traind = traind[(traind[\"text_clean\"] != \"\") & (traind[\"text_clean\"] != \"null\")]\n",
    "\n",
    "traind = traind.dropna(\n",
    "    axis=\"index\", subset=[\"label\", \"text\", \"text_clean\"]\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18f10bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A group of friends began to volunteer at a hom...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>group friend began volunt homeless shelter nei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>British Prime Minister @Theresa_May on Nerve A...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>british prime minist theresa may nerv attack f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In 1961, Goodyear released a kit that allows P...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>goodyear releas kit allow ps brought heel http...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  A group of friends began to volunteer at a hom...    0.0   \n",
       "1  British Prime Minister @Theresa_May on Nerve A...    0.0   \n",
       "2  In 1961, Goodyear released a kit that allows P...    0.0   \n",
       "\n",
       "                                          text_clean  \n",
       "0  group friend began volunt homeless shelter nei...  \n",
       "1  british prime minist theresa may nerv attack f...  \n",
       "2  goodyear releas kit allow ps brought heel http...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_clean = traind.copy() #making a copy of the training data set\n",
    "train_data_clean.head(3) #viewwing the first 3 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c76cd33d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stargazer</td>\n",
       "      <td>stargaz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yeah</td>\n",
       "      <td>yeah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PD: Phoenix car thief gets instructions from Y...</td>\n",
       "      <td>pd phoenix car thief get instruct youtub video</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "id                                                      \n",
       "0                                          stargazer    \n",
       "1                                                yeah   \n",
       "2   PD: Phoenix car thief gets instructions from Y...   \n",
       "\n",
       "                                        text_clean  \n",
       "id                                                  \n",
       "0                                          stargaz  \n",
       "1                                             yeah  \n",
       "2   pd phoenix car thief get instruct youtub video  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_clean = testd.copy() #making a copy of the testing data set\n",
    "test_data_clean.head(3)#viewwing the first 3 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b14d200d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year     4129\n",
       "one      3285\n",
       "like     3128\n",
       "new      2998\n",
       "look     2847\n",
       "         ... \n",
       "star      565\n",
       "ad        564\n",
       "ago       563\n",
       "came      563\n",
       "elect     563\n",
       "Length: 200, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bokeh.models import NumeralTickFormatter\n",
    "# viewing Word Frequency of most common words\n",
    "word_freq = pd.Series(\" \".join(train_data_clean[\"text_clean\"]).split()).value_counts()\n",
    "word_freq[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60e01bf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hatsun</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nfler</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hicock</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mccall</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wahr</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  freq\n",
       "0  hatsun     1\n",
       "1   nfler     1\n",
       "2  hicock     1\n",
       "3  mccall     1\n",
       "4    wahr     1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list most 5 uncommon words\n",
    "word_freq[-5:].reset_index(name=\"freq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8f6c45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    0.538221\n",
       "1.0    0.461779\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribution of ratings\n",
    "train_data_clean[\"label\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6de84153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data into x and y for training data and x for testing data\n",
    "Xtr=train_data_clean['text_clean']\n",
    "Ytr=train_data_clean['label']\n",
    "test_data_clean=testd['text_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3dd77b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Debi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass input=word as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline #Pipeline of transforms with a final estimator.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #to Convert text to a matrix of TF-IDF features.\n",
    "from sklearn.linear_model import LogisticRegression #importing the logistic regression model\n",
    "\n",
    "\n",
    "pipe = Pipeline([(\"tfidf\", TfidfVectorizer(\"word\")), (\"lr\", LogisticRegression())]) #we will use the word vectorizer and the logistic regrression model\n",
    "\n",
    "\n",
    "params = {\n",
    "    \n",
    "    \"tfidf__ngram_range\": [(1, 4),(2,5)],#n-gram range:tuple (min_n, max_n), The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used.\n",
    "    \"tfidf__max_df\": np.arange(0.6, 0.8),#When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).\n",
    "    \"tfidf__min_df\": np.arange(5,30 ),#When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. \n",
    "    'lr__penalty' : ['l1','l2'], #Penalized logistic regression imposes a penalty to the logistic model for having too many variables. This results in shrinking the coefficients of the less contributive variables toward zero. \n",
    "    'lr__C' : np.logspace(-3,3,30), #C value controls the strength of the penalty\n",
    "    'lr__solver': [ 'liblinear','newton-cg', 'lbfgs'] #setting solver options/ranges to liblinear,newton-cg,lbfgs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2cf238dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 10 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Debi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.68153394 0.81905744 0.61759491        nan        nan        nan\n",
      " 0.63161273 0.65247247 0.70316885        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8190574376210747\n",
      "best hyperparameter {'tfidf__ngram_range': (1, 4), 'tfidf__min_df': 25, 'tfidf__max_df': 0.6, 'lr__solver': 'liblinear', 'lr__penalty': 'l2', 'lr__C': 0.0016102620275609393}\n"
     ]
    }
   ],
   "source": [
    "# now using random search with valisation set\n",
    "#Further split the original training set to a train and a validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    Xtr, Ytr, train_size = 0.7, stratify = Ytr, random_state = 45)#the random_state parameter is used for initializing the internal random number generator\n",
    "#The stratify option instructs sklearn to divide the dataset into a test and training set with the ratio of class labels in the variable supplied.\n",
    "\n",
    "# Create a list where train data indices are -1 and validation data indices are 0\n",
    "# X_train2 (new training set), X_train\n",
    "split_index = [-1 if x in X_train.index else 0 for x in Xtr.index]\n",
    "\n",
    "# Use the list to create PredefinedSplit\n",
    "pds = PredefinedSplit(test_fold = split_index)\n",
    "#\n",
    "random_search_lr = RandomizedSearchCV(\n",
    "    pipe, params, cv=pds, verbose=1, n_jobs=2, \n",
    "    # number of random trials\n",
    "    n_iter=10,\n",
    "    scoring='roc_auc')#The degree of separability/distinction or intermingling/crossover between the forecasts of the two classes is shown by the ROC-AUC.\n",
    "\n",
    "random_search_lr.fit(Xtr,Ytr)  #fitting my data into random_search_lr results\n",
    "\n",
    "print('best score {}'.format(random_search_lr.best_score_)) #printing best scores\n",
    "print('best hyperparameter {}'.format(random_search_lr.best_params_)) #printing best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e55b1e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame() # creating dataframe for the submission\n",
    "\n",
    "submission['id'] = test_data_clean.index #using the column index of x test values to fill submission['id'] column\n",
    "\n",
    "submission['label'] = random_search_lr.predict_proba(test_data_clean)[:,1] #predicting the probabilities and filling submission['label'] column with their values\n",
    "\n",
    "submission.to_csv('predwalkthrough.csv', index=False)#generating the submission file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e70e89",
   "metadata": {},
   "source": [
    "# Trial one using logistic regression classifier with random search and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da134068",
   "metadata": {},
   "source": [
    "# Expectaion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f0c866",
   "metadata": {},
   "source": [
    "I expect to get the optimal hyperparamters that will give me the best performance and highest accuracy using logistic regression model with random search and  validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f1dfa3",
   "metadata": {},
   "source": [
    "# Observation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989faee5",
   "metadata": {},
   "source": [
    "After running the code,the best hyperparameters that was defined were : \n",
    "    \n",
    "'tfidf__ngram_range': (1, 4)\n",
    "    \n",
    "'tfidf__min_df': 25\n",
    "    \n",
    "'tfidf__max_df': 0.6\n",
    "    \n",
    "'lr__solver': 'liblinear'\n",
    "    \n",
    "'lr__penalty': 'l2'\n",
    "    \n",
    "'lr__C': 0.0016102620275609393\n",
    "\n",
    "kaggle score 0.83008\n",
    "\n",
    "best score 0.8190574376210747"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef7c911",
   "metadata": {},
   "source": [
    "# Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4dcbbf",
   "metadata": {},
   "source": [
    "I will change my vectorizer to character level vectorizer and for my hyperparameters, I will change their ranges to \n",
    "\n",
    "'lr__penalty' : [‘l1’, ‘l2’, ‘elasticnet’, ‘none’]\n",
    "\n",
    " 'lr__C' : np.logspace(-6,6,40)\n",
    " \n",
    " 'lr__solver': [ 'liblinear','newton-cg', ‘sag’, ‘saga’]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b925dfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \n",
    "    \"tfidf__ngram_range\": [(1, 4),(2,5)], #n-gram range:tuple (min_n, max_n), The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used.\n",
    "    \"tfidf__max_df\": np.arange(0.6, 0.8), #When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).\n",
    "    \"tfidf__min_df\": np.arange(5,30 ),#When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. \n",
    "    'lr__penalty' : ['l1', 'l2', 'elasticnet', 'none'],#Penalized logistic regression imposes a penalty to the logistic model for having too many variables. This results in shrinking the coefficients of the less contributive variables toward zero. \n",
    "    'lr__C' : np.logspace(-6,6,40), #C value controls the strength of the penalty\n",
    "    'lr__solver': [ 'liblinear','newton-cg', 'sag', 'saga']#setting solver options/ranges to liblinear,newton-cg,lbfgs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca01640d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 10 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Debi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.84789151        nan        nan        nan        nan 0.85562872\n",
      "        nan        nan        nan 0.68890488]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8556287210249962\n",
      "best hyperparameter {'tfidf__ngram_range': (1, 4), 'tfidf__min_df': 16, 'tfidf__max_df': 0.6, 'lr__solver': 'saga', 'lr__penalty': 'l1', 'lr__C': 58780.16072274924}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Debi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "random_search_lr = RandomizedSearchCV(\n",
    "    pipe, params, cv=pds, verbose=1, n_jobs=2, \n",
    "    # number of random trials\n",
    "    n_iter=10,\n",
    "    scoring='roc_auc')#The degree of separability/distinction or intermingling/crossover between the forecasts of the two classes is shown by the ROC-AUC.\n",
    "\n",
    "random_search_lr.fit(Xtr,Ytr)  #fitting my data into random_search_lr results\n",
    "\n",
    "print('best score {}'.format(random_search_lr.best_score_)) #printing best score\n",
    "print('best hyperparameter {}'.format(random_search_lr.best_params_)) #printing best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ef3a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame() # creating dataframe for the submission\n",
    "\n",
    "submission['id'] = test_data_clean.index #using the column index of x test values to fill submission['id'] column\n",
    "\n",
    "submission['label'] = random_search_lr.predict_proba(test_data_clean)[:,1] #predicting the probabilities and filling submission['label'] column with their values\n",
    "\n",
    "submission.to_csv('predwalkthrough.csv', index=False)#generating the submission file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666c7b81",
   "metadata": {},
   "source": [
    "# Trial two using logistic regression classifier with random search and validation set but with different hyperparameters ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a53b884",
   "metadata": {},
   "source": [
    "# Expectaion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d962985",
   "metadata": {},
   "source": [
    "I expected to get a higher score after changing the ranges of my hyper parammetrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e599cde",
   "metadata": {},
   "source": [
    "# Observation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f57b5ab",
   "metadata": {},
   "source": [
    "After running the code,the best hyperparameters that was defined were : \n",
    "\n",
    "'tfidf__ngram_range': (1, 4)\n",
    "\n",
    "'tfidf__min_df': 16\n",
    "\n",
    "'tfidf__max_df': 0.6\n",
    "\n",
    "'lr__solver': 'sag'\n",
    "\n",
    "'lr__penalty': 'l1'\n",
    "\n",
    "'lr__C': 58780.16072274924\n",
    "\n",
    "kaggle score 0.83286\n",
    "\n",
    "best score 0.8556287210249962\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d132254",
   "metadata": {},
   "source": [
    "# Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511277b2",
   "metadata": {},
   "source": [
    "I'm going to use the xgboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bdc40718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91cb758f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Debi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass input=char as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([(\"tfidf\", TfidfVectorizer(\"char\")),  ('xgb_model',xgb.XGBClassifier())])\n",
    "\n",
    "params = {\n",
    "    \n",
    "    \"tfidf__ngram_range\": [(1, 4),(2,5)], # #n-gram range:tuple (min_n, max_n), The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used.\n",
    "    \"tfidf__max_df\": np.arange(0.6, 0.8),#When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).\n",
    "    \"tfidf__min_df\": np.arange(5,30),#When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. \n",
    "    'xgb_model__min_child_weight': [1, 5, 10], #The number of samples required to form a leaf node (the end of a branch)\n",
    "    'xgb_model__gamma': [0.5, 1, 1.5, 2, 5],#The gamma is an unbounded parameter from 0 to infinity that is used to control the model’s tendency to overfit.\n",
    "    'xgb_model__subsample': [0.6, 0.8, 1.0], # determines how much of the initial dataset is fair game for random sampling during each iteration of the boosting process.\n",
    "    'xgb_model__colsample_bytree': [0.6, 0.8, 1.0],#defines percentage of features will be used for building each tree.\n",
    "    'xgb_model__max_depth': [3, 4, 5]# how deep each estimator is permitted to build a tree.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "517479e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 10 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Debi\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:57:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "best score 0.8206981714790174\n",
      "best hyperparameter {'xgb_model__subsample': 0.8, 'xgb_model__min_child_weight': 1, 'xgb_model__max_depth': 4, 'xgb_model__gamma': 5, 'xgb_model__colsample_bytree': 1.0, 'tfidf__ngram_range': (1, 4), 'tfidf__min_df': 16, 'tfidf__max_df': 0.6}\n"
     ]
    }
   ],
   "source": [
    "random_search_xg = RandomizedSearchCV(\n",
    "    pipe, params, cv=pds, verbose=1, n_jobs=2, \n",
    "    # number of random trials\n",
    "    n_iter=10,\n",
    "    scoring='roc_auc')#The degree of separability/distinction or intermingling/crossover between the forecasts of the two classes is shown by the ROC-AUC.\n",
    "\n",
    "random_search_xg.fit(Xtr,Ytr) #fitting my x and y into the random search results\n",
    "\n",
    "print('best score {}'.format(random_search_xg.best_score_)) #printing  best scores\n",
    "print('best hyperparameter {}'.format(random_search_xg.best_params_)) #printing best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c5edee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame() # creating dataframe for the submission\n",
    "\n",
    "submission['id'] = test_data_clean.index #using the column index of x test values to fill submission['id'] column\n",
    "\n",
    "submission['label'] = random_search_xg.predict_proba(test_data_clean)[:,1] #predicting the probabilities and filling submission['label'] column with their values\n",
    "\n",
    "submission.to_csv('predwalkthrough.csv', index=False)#generating the submission file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4438da",
   "metadata": {},
   "source": [
    "# Trial three using xgboost model with random search and validation set  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3951c24",
   "metadata": {},
   "source": [
    "# Expectaion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4497ad",
   "metadata": {},
   "source": [
    "I expected to get a higher score after changing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2f62d1",
   "metadata": {},
   "source": [
    "# Observation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52678807",
   "metadata": {},
   "source": [
    "After running the code,the best hyperparameters that was defined were : \n",
    "\n",
    "'xgb_model__subsample': 0.8\n",
    "\n",
    "'xgb_model__min_child_weight': 1\n",
    "\n",
    "'xgb_model__max_depth': 4\n",
    "\n",
    "'xgb_model__gamma': 5\n",
    "\n",
    "'xgb_model__colsample_bytree': 1.0\n",
    "\n",
    "'tfidf__ngram_range': (1, 4)\n",
    "\n",
    "'tfidf__min_df': 16\n",
    "\n",
    "'tfidf__max_df': 0.6\n",
    "\n",
    "kaggle score Score: 0.77052\n",
    "\n",
    "best score 0.8206981714790174"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f6707e",
   "metadata": {},
   "source": [
    "# Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe399391",
   "metadata": {},
   "source": [
    "I will change the hyperparameters' ranges to the xgboost model and I will use the word level vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18063e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Debi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass input=word as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([(\"tfidf\", TfidfVectorizer(\"word\")),  ('xgb_model',xgb.XGBClassifier())])\n",
    "\n",
    "params = {\n",
    "    \n",
    "    \"tfidf__ngram_range\": [(3, 6),(4,8)],#n-gram range:tuple (min_n, max_n), The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used.\n",
    "    \"tfidf__max_df\": np.arange(0.3, 0.9),#When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).\n",
    "    \"tfidf__min_df\": np.arange(8,24),#When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature.\n",
    "    'xgb_model__min_child_weight': [3, 6, 11], #The number of samples required to form a leaf node (the end of a branch)\n",
    "    'xgb_model__gamma': [0.25, 0.75, 2, 2.5, 5],#The gamma is an unbounded parameter from 0 to infinity that is used to control the model’s tendency to overfit.\n",
    "    'xgb_model__subsample': [0.2, 0.5, 0.6], # determines how much of the initial dataset is fair game for random sampling during each iteration of the boosting process.\n",
    "    'xgb_model__colsample_bytree': [0.6, 0.8, 1.0],#defines percentage of features will be used for building each tree.\n",
    "    'xgb_model__max_depth': [3, 6, 9]# how deep each estimator is permitted to build a tree.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d10e95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 10 candidates, totalling 10 fits\n",
      "[19:58:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "best score 0.5076353269477214\n",
      "best hyperparameter {'xgb_model__subsample': 0.6, 'xgb_model__min_child_weight': 6, 'xgb_model__max_depth': 6, 'xgb_model__gamma': 2, 'xgb_model__colsample_bytree': 0.8, 'tfidf__ngram_range': (3, 6), 'tfidf__min_df': 18, 'tfidf__max_df': 0.3}\n"
     ]
    }
   ],
   "source": [
    "random_search_xg = RandomizedSearchCV(\n",
    "    pipe, params, cv=pds, verbose=1, n_jobs=2, \n",
    "    # number of random trials\n",
    "    n_iter=10,\n",
    "    scoring='roc_auc')\n",
    "\n",
    "random_search_xg.fit(Xtr,Ytr) #fitting my x and y into the xgboost random search rresults\n",
    "\n",
    "print('best score {}'.format(random_search_xg.best_score_)) #printing best scores\n",
    "print('best hyperparameter {}'.format(random_search_xg.best_params_)) #printing best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9658d7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame() # creating dataframe for the submission\n",
    "\n",
    "submission['id'] = test_data_clean.index #using the column index of x test values to fill submission['id'] column\n",
    "\n",
    "submission['label'] = random_search_xg.predict_proba(test_data_clean)[:,1] #predicting the probabilities and filling submission['label'] column with their values\n",
    "\n",
    "submission.to_csv('predwalkthrough.csv', index=False)#generating the submission file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708f1a62",
   "metadata": {},
   "source": [
    "# Trial four using xgboost model with random search and validation set but with different hyperparameters ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93449d63",
   "metadata": {},
   "source": [
    "# Expectaion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0043203d",
   "metadata": {},
   "source": [
    "I expected to get a higher score after changing the hyper parameters' ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46243da",
   "metadata": {},
   "source": [
    "# Observation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd676cf",
   "metadata": {},
   "source": [
    "After running the code,the best hyperparameters that was defined were : \n",
    "\n",
    "'xgb_model__subsample': 0.6\n",
    "\n",
    "'xgb_model__min_child_weight': 6\n",
    "\n",
    "'xgb_model__max_depth': 6\n",
    "\n",
    "'xgb_model__gamma': 2\n",
    "\n",
    "'xgb_model__colsample_bytree': 0.8\n",
    "\n",
    "'tfidf__ngram_range': (3, 6)\n",
    "\n",
    "'tfidf__min_df': 18\n",
    "\n",
    "'tfidf__max_df': 0.3\n",
    "\n",
    "kaggle score Score: 0.50315\n",
    "\n",
    "best score 0.5076353269477214"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217022ca",
   "metadata": {},
   "source": [
    "# Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dac14e",
   "metadata": {},
   "source": [
    "I will change my model to the randomforest () and I will use char level vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "943dbb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipe = Pipeline([(\"tfidf\", TfidfVectorizer(\"word\")),  ('my_classifier',RandomForestClassifier())])\n",
    "\n",
    "params = {\n",
    "    \n",
    "    \"tfidf__ngram_range\": [(3, 6),(4,8)],#n-gram range:tuple (min_n, max_n), The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used.\n",
    "    \"tfidf__max_df\": np.arange(0.3, 0.9),#When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).\n",
    "    \"tfidf__min_df\": np.arange(8,24),#When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature.\n",
    "    'my_classifier__n_estimators': [50, 100, 150],  # ranges of n_estimators which are the number of trees to be used in the forest.\n",
    "    # I set my  n_estimators ranges to [50, 100, 150]\n",
    "    'my_classifier__max_depth':[30, 60, 90]  \n",
    "    # I set my  max depth ranges to [30, 60, 90] which are The number of splits that each decision tree is allowed to make.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de041c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 10 candidates, totalling 10 fits\n",
      "best score 0.5185222470697569\n",
      "best hyperparameter {'tfidf__ngram_range': (3, 6), 'tfidf__min_df': 11, 'tfidf__max_df': 0.3, 'my_classifier__n_estimators': 100, 'my_classifier__max_depth': 60}\n"
     ]
    }
   ],
   "source": [
    "random_search_rnf = RandomizedSearchCV(\n",
    "    pipe, params, cv=pds, verbose=1, n_jobs=2, \n",
    "    # number of random trials\n",
    "    n_iter=10,\n",
    "    scoring='roc_auc')\n",
    "\n",
    "random_search_rnf.fit(Xtr,Ytr)\n",
    "\n",
    "print('best score {}'.format(random_search_rnf.best_score_))\n",
    "print('best hyperparameter {}'.format(random_search_rnf.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d09609b",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame() # creating dataframe for the submission\n",
    "\n",
    "submission['id'] = test_data_clean.index #using the column index of x test values to fill submission['id'] column\n",
    "\n",
    "submission['label'] = random_search_rnf.predict_proba(test_data_clean)[:,1] #predicting the probabilities and filling submission['label'] column with their values\n",
    "\n",
    "submission.to_csv('predwalkthrough.csv', index=False)#generating the submission file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99382a4f",
   "metadata": {},
   "source": [
    "# Trial five using randomforest() model with random search and validation set "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b21fc9",
   "metadata": {},
   "source": [
    "# Expectaion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d605a13c",
   "metadata": {},
   "source": [
    "I expected to get a higher score after changing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc4d582",
   "metadata": {},
   "source": [
    "# Observation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cdc6c9",
   "metadata": {},
   "source": [
    "After running the code,the best hyperparameters that was defined were : \n",
    "\n",
    "'tfidf__ngram_range': (3, 6)\n",
    "\n",
    "'tfidf__min_df': 11\n",
    "\n",
    "'tfidf__max_df': 0.3\n",
    "\n",
    "'my_classifier__n_estimators': 100\n",
    "\n",
    "'my_classifier__max_depth': 60\n",
    "\n",
    "score on kaggle :  0.50691\n",
    "\n",
    "best score 0.5185222470697569"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c960f0",
   "metadata": {},
   "source": [
    "# Refrences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ccf327",
   "metadata": {},
   "source": [
    "https://pdf.sciencedirectassets.com/277811/1-s2.0-S1877042813X0028X/1-s2.0-S1877042813041918/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEOb%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQCjejOQyKdO7U33cK6CHIkH%2F5RcEs9esfrECmpPNqnZxwIgDXTl6duLcDQMHQ9xBtE%2BGp4O9%2B33gQRMH2yhqFfCRokq%2BgMIXhAEGgwwNTkwMDM1NDY4NjUiDPcCm7JZf8j%2F2SKTLirXA41KPsw6b3nl8BuptqiheyReIhuIAACodyueVBEn3xA977Noz2ZIbVQgnOlzvQ8jhgvOgqk%2FN6XjbVN7ErmCqffpY1p%2FO31NoiKT7zKoNw9k11blK1hPNt%2B%2FpnH926DnCfVP%2FxqvFvqRM%2FjWJwL2ko%2B6iiIZrrbmbToCVGcbNxLC1lNjeZEAhDn%2BWg1qfmnOd6xROkRa1LtTN13XJKBE3DYpQkHsIkO%2B1bjqzqKmgGZ6F2FWFdTgFB0bfLR2QoOGKoSLi1psJ%2FVKO1ODMZNIkjdnJgjEfzyw09Zl80qA%2F0FufLouJFTkwnLJGDRfH5VtRVyHVTs8nZ7q7j2Pg%2BuCygnfst0blSsHVgiddtQwW302dYDHlSw15%2FixfPJkNgGI9Cr6tu4yi3mnsg9MitXzAi8Hr9JWkMoqhFePKuSx%2FVlCW0OVjmecjuEgH7mU4Lv5LflUFSQbvbuJ07D%2BsHpfMk%2BxvBAu2DUaxxkrVofZSHX%2BDmB0Jy7C2ES5If9ov6HilVHIYDXP6YW4QacHuOV90KEhB%2FRnajd1I8OA%2BvZbtPay318M%2FntL8NLOPFCLXnZtwKH1zC54PQS1kSL8RLJSeIQ%2BEUw04sk1FhzOjzDjPnWPMGCUkhi%2F%2FzC7%2FbyRBjqlASY0PYfL9%2FFb3BPo%2F3F9AG9NvNQY%2BFC2vSvoX7cwe0X1mNW3S2ntFwNRahsiDYk%2BkM4%2FwcwPf%2B9ATO0%2BA1ityvdAdFmK5YEERp09H7b2bXJWfBc8uY2udNQEvsHOscNwnjZtL9jX5QQeGVtxSJzD4Ptu61FVgvP0S315XMrTu8GvyaDQ45PQAUIbR849GvUAVm3Hw7pUx9Yn4jHS%2FFfSV5MP0ar0Ag%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20220314T133850Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYUPZR4G5W%2F20220314%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=da106f5b7265022db1ea709957cd21b63b435910ca1fd39fffaa848dacca37fa&hash=f595daebc10953250ebbd13f1925d859be6341035ceacba5fbea2152b96975b8&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1877042813041918&tid=spdf-3cf4728f-3ded-491c-ae14-58a482dd28c7&sid=cd3ce52c85b4774e1a0a6db1bb4f7b548184gxrqb&type=client&ua=4c00030b595b50535f&rr=6ebd69761e905fc5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad85f7f",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405e3159",
   "metadata": {},
   "source": [
    "https://charmestrength.com/what-is-penalty-in-logistic-regression/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1ff557",
   "metadata": {},
   "source": [
    "https://kevinvecmanis.io/machine%20learning/hyperparameter%20tuning/dataviz/python/2019/05/11/XGBoost-Tuning-Visual-Guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a1f718",
   "metadata": {},
   "source": [
    "https://datascience.stackexchange.com/questions/25581/what-is-the-difference-between-countvectorizer-token-counts-and-tfidftransformer#:~:text=The%20only%20difference%20is%20that%20the%20TfidfVectorizer%20%28%29,%28%29%20assigns%20a%20score%20while%20CountVectorizer%20%28%29%20counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb21f51f",
   "metadata": {},
   "source": [
    "https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html#:~:text=This%20guarantees%20that%20a%20sequence,the%20document%20to%20be%20known."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502bd344",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2021/09/what-are-n-grams-and-how-to-implement-them-in-python/\n",
    "\n",
    "https://help.monkeylearn.com/en/articles/2174105-n-gram-range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b58a04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
